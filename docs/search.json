[
  {
    "objectID": "index.html#latest-written-projects",
    "href": "index.html#latest-written-projects",
    "title": "Boon Chin Look, BBS",
    "section": "Latest Written Projects",
    "text": "Latest Written Projects\n\n\n\n\n\n\n\n\n\n\nThe Solow Model\n\n\n\nEconomics\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis Comparison Modernization and Dependency Theories\n\n\n\nEconomics\n\n\n\n\n\n\n\nNov 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBotswana’s Economic Transformation through mineral wealth\n\n\n\nEconomics\n\n\nPresentation\n\n\n\n\n\n\n\nOct 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the Impact of Technological Change and Shifting Consumer Behaviour on the Global Automobile Industry\n\n\n\nEconomics\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an Ethical Theory\n\n\n\nEthics\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-code-projects",
    "href": "index.html#latest-code-projects",
    "title": "Boon Chin Look, BBS",
    "section": "Latest Code Projects",
    "text": "Latest Code Projects\n\n\n\n\n\n\n\n\n\n\nePortfolio (this website)\n\n\n\nQuarto\n\n\nHTML\n\n\nSCSS\n\n\n\n\n\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct Proposal\n\n\n\nPython\n\n\n\n\n\n\n\nApr 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript Project: Interactive Fruit Shop Basket\n\n\n\nJavaScript\n\n\n\n\n\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Project - Restaurant Order Management Database\n\n\n\nSQL\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "reports/solow_model/index.html",
    "href": "reports/solow_model/index.html",
    "title": "The Solow Model",
    "section": "",
    "text": "This report reflects on the Solow Model of economic growth, exploring its key insights and limitations. It highlights the model’s focus on capital accumulation, labour, and technological progress as primary drivers of growth, while also addressing its shortcomings, such as the exogenous nature of technological progress and limited consideration of trade, inequality, and human capital. By integrating academic literature, this report offers a nuanced critique of the model’s theoretical foundations and its practical relevance in today’s economic context.\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/index.html",
    "href": "reports/index.html",
    "title": "Written Projects (Reports)",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Solow Model\n\n\n\n\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\nBoon Chin Look\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis Comparison Modernization and Dependency Theories\n\n\n\n\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\nBoon Chin Look\n\n\n\n\n\n\n\n\n\n\n\n\nBotswana’s Economic Transformation through mineral wealth\n\n\n\n\n\n\nEconomics\n\n\nPresentation\n\n\n\n\n\n\n\n\n\nOct 20, 2024\n\n\nAssa Babissagana, Boon Chin Look, Daniela Groza, Fatima Contreiras\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the Impact of Technological Change and Shifting Consumer Behaviour on the Global Automobile Industry\n\n\n\n\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nBoon Chin Look\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an Ethical Theory\n\n\n\n\n\n\nEthics\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nBoon Chin Look, Jack MacNamara\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "reports/botswana's_economic_transformation/index.html",
    "href": "reports/botswana's_economic_transformation/index.html",
    "title": "Botswana’s Economic Transformation through mineral wealth",
    "section": "",
    "text": "This group project involved writing a report and producing a summary video presentation examining how Botswana successfully leveraged its mineral wealth, particularly diamonds, to transform from one of the world’s poorest nations into a stable, middle-income economy. The analysis highlights the government’s effective resource management, investments in social services, and infrastructure development.\nThe study also explored the risks of overreliance on diamonds and the growing impact of synthetic alternatives, emphasizing Botswana’s strategic shift toward economic diversification. Through initiatives like Vision 2036 and the Economic Reform and Transformation Plan, the country is promoting growth in tourism, agriculture, and manufacturing to build a more resilient and inclusive economy.\n\n\n\n Back to top"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experiences & growth",
    "section": "",
    "text": "Throughout my time at university, I’ve actively sought opportunities to grow both personally and professionally through diverse experiences in work, volunteering, and sport.\nAs a Business with Data Analytics student at DCU, I’ve taken on leadership roles such as Class Representative and DCU Mentor, while also being a mentee myself. I love supporting my peers and fostering a strong sense of community. I’m always open to opportunities that involve learning, growing, and helping others.\nMy commitment to giving back is evident in my support for colleagues when needed, and through initiatives like Ballymunch, where I help adults learn to cook. I also completed the Healthy Food Made Easy programme, an initiative led by the Health Service Executive (HSE) in my local community.\nDriven by a passion for continuous learning, I participated in Dell Technologies’ STEM Aspire and Innovative Communities mentorship, while my job has helped me develop strong time management skills by balancing work with academic and extracurricular commitments. Outside of academics, I challenge myself every year by joining clubs outside my comfort zone, from rock climbing and art to table tennis. Most recently, I completed the 11km Runamuck obstacle course. These experiences have shaped me into a well-rounded, motivated individual with a strong work ethic and a drive to make a positive impact.\n\n\n  My Interests\n  + \n\n\n  \n    \n        \n          \n        \n        \n          \n            Running\n            Fitness\n          \n          Last year, I organized and participated in a 11km obstacle course (Runamuck Challenge) with minimal training, an experience that was both challenging and rewarding.\n          Since then, I’ve committed to regular running and gym workouts to improve my endurance. This year, I’m taking on the 13km Hell & Back obstacle course, better prepared and encouraging more colleagues to take on the challenge with me.\n          October 2024-Present, Dublin, Ireland\n        \n      \n    \n        \n          \n        \n        \n          \n            Baking & Cooking\n            Passion\n          \n          One year, I challenged myself to learn baking and cooking, which inspired me to join the Ballymunch volunteering programme, supported by the Raising & Giving (RAG) Society.\n          Through this experience, I developed valuable skills and later joined the HSE’s Healthy Food Made Easy programme, a six-week initiative with weekly four-hour sessions. I was part of a class of about 11 people from my local community, where we cooked nutritious meals together and built a supportive environment based on shared learning.\n          January 2023-Present, Dublin, Ireland\n        \n      \n    \n      \n        \n      \n      \n        \n          Gym\n          Fitness & Well-being\n        \n        Regular physical activity is essential for maintaining both physical and mental health. I go to the gym at least twice a week, not only to stay fit but also to relieve stress and clear my mind. For me, the gym is a space for personal growth and well-being.\n        December 2022-Present, Dublin, Ireland\n      \n    \n    \n      \n        \n      \n      \n        \n          Rock Climbing\n          Competitive Sport\n        \n        I am an active member of the DCU Rock Climbing Club. Bouldering, climbing up to 2 meters high and descending safely, helps me step out of my comfort zone and meet like-minded individuals within a sport.\n        Additionally, in 2024, as part of DCU rock climbing, I competed in the intervarsities and placed 3rd in the beginner female category.\n        September 2022-Present, Dublin, Ireland\n      \n    \n  \n\n\n\n  Work Experience\n  + \n\n\n    \n    \n      \n        \n      \n      \n        \n          The Baby Academy Ltd.,\n          Webinar Moderator\n        \n        Main tasks\n        \n  Prepared online classes, provided troubleshooting support, and assisted instructors to ensure smooth sessions.\n  Managed administrative tasks in Excel, including tracking class attendance.\n  Moderated online classes with up to 500 participants.\n\n\n        April 2024-Present, Dublin, Ireland\n      \n    \n    \n      \n        \n      \n      \n        \n          KPMG\n          Financial Services Tax Intern\n        \n        Main tasks\n        \n  Assisted team members and managers with client work, research, and organizational tasks.\n  Utilized Office 365 to manage data in Excel, schedule meetings in Outlook, and communicate with colleagues via email.\n\n\n        June 2023-July 2023, Dublin, Ireland\n      \n    \n    \n      \n        \n      \n      \n        \n          Circle K\n          Customer Representative\n        \n        Main tasks\n        \n  Worked at the tills dealing with customers for fuel and items.\n  Cleaned fuel pumps and stocked the shop.\n\n\n        June 2021-January 2022, Dublin, Ireland\n      \n    \n  \n\n\n\n  Local & Community Involvement\n  + \n\n\n    \n      \n        \n          \n        \n        \n          \n            Experience Japan\n            Community Involvement\n          \n          Driven by a strong commitment to volunteering, I had the privilege of working as and contributing to Experience Japan as an Area Leader in the Kitchen Cabinet crew. \n          In this role, I worked alongside dedicated teams focused on two key areas: managing the storage and changing spaces for performers and efficiently reallocating volunteer resources throughout the event. This was a rewarding opportunity to support a vibrant cultural initiative while collaborating with an incredible group of volunteers to ensure a seamless experience for everyone at the event.\n          April 2025-April 2025, Dublin, Ireland\n        \n      \n    \n      \n        \n      \n      \n        \n          Healthy Food Made Easy\n          Local community involvement\n        \n        Participating in the 'Healthy Food Made Easy: A Community Cooking Programme,' a 4-hour-per-week initiative spanning six weeks, was a truly enriching experience.\n        It not only provided me with the opportunity to engage with diverse individuals within my community but also facilitated unexpected reconnections with people I hadn't seen in years. This experience served as a valuable reminder of how life's paths can cross again in the most surprising ways.\n        January 2025-February 2025, Dublin, Ireland\n      \n    \n      \n        \n          \n        \n        \n          \n            DCU Mentor\n            College Involvement\n          \n          As a mentor at Dublin City University, I had the privilege of supporting a group of Business Studies students as they navigated their transition into college life.\n          Through structured teamwork exercises, engaging icebreakers, and collaborative activities, I fostered a welcoming and inclusive environment that encouraged confidence and connection. My role was to ensure that each student felt comfortable, supported, and empowered as they adapted to this new academic and social experience.\n          September 2024-April 2025, Dublin, Ireland\n        \n      \n     \n        \n          \n        \n        \n          \n            Ballymunch\n            Volunteering\n          \n          Participating in Ballymunch, a cooking class initiative for parents in Ballymun, has been an invaluable experience. Supported by DCU's Raising & Giving (RAG) Society, this volunteering opportunity has not only deepened my understanding of how to prepare affordable, nutritious meals but has also equipped me with essential skills such as effective communication, teamwork, and collaboration.\n          This opportunity has allowed me to engage with my community, share knowledge, and develop practical skills that extend beyond the kitchen and college.\n          November 2023-Present, Dublin, Ireland\n        \n      \n      \n        \n          \n        \n        \n          \n            Class Representative & DCU Access Mentor\n            College Involvement\n          \n          Engaging with the college community as a DCU Access Mentor and Class Representative enriched my college experience in numerous ways. These roles not only allowed me to immerse myself in various aspects of college life but also helped me develop a balance between academic responsibilities and collaborative contributions to the student body.\n          Working alongside and interacting with fellow students provided a rewarding opportunity to connect on a deeper level, especially as I could empathize with their journey, having once been in their position.\n          September 2023-April 2024, Dublin, Ireland\n        \n      \n  \n\n\n\n  Licenses & Certifications\n  + \n\n\n  \n    \n      \n        \n      \n      \n        \n          Irish Driving license\n          Commitment\n        \n         With determination and careful planning, I earned my Irish driving license without access to a family car, organizing my own practice sessions around a busy college and work schedule. This experience taught me persistence and adaptability.\n         \n         Moreover, the support and encouragement from my community played a vital role, reminding me that while independence is valuable, resilience is often reinforced by the support and encouragement of those around us.\n        October 2023-April 2025, Dublin, Ireland\n      \n    \n    \n      \n        \n      \n      \n        \n          Pearson Vue ITs\n          Software Development\n        \n        Through Cenit College, I successfully completed the Pearson VUE certifications in Python and Software Development. This educational journey provided me with hands-on experience in programming languages such as C#, Python, and SQL. \n        Additionally, I had the opportunity to collaborate with my classmates on various projects, fostering teamwork and enhancing my practical knowledge in software development.\n        November 2023-August 2024, Dublin, Ireland\n      \n    \n    \n      \n        \n      \n      \n        This Section showcased the certificates that spanned over months and years.\n        For shorter courses I have done, check out Professional Development!\n      \n    \n  \n\n\n\nLearn more about my future goals →\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/sql_project/index.html",
    "href": "code_projects/sql_project/index.html",
    "title": "SQL Project - Restaurant Order Management Database",
    "section": "",
    "text": "This project was developed as part of an 8-week introduction to the structured query language (SQL) course hosted by Girls Code First. Throughout the course, I built foundational SQL skills through hands-on learning and completed an individual assignment that covered topics such as:\n\nSQL coding and database management techniques.\nData analysis and manipulation\nWorking with complex data structures\nDatabase design and data visualization\nProject presentation and reporting\n\n\n\n\n\n\nFinal Project: SQL ERD – Restaurant Order Management\nFor my final project, I designed a relational database centred around restaurant order management. This involved implementing key database concepts such as primary and foreign keys, and developing an Enhanced Entity-Relationship (EER) diagram to visually represent the system structure.\nThe project gave me the opportunity to apply theoretical concepts in a real-world scenario, significantly enhancing both my technical proficiency and confidence in structured data management. Below, you can find code snippets used to create the database tables and their relationships.\n-- Creates the restaurant Database\nCreate Database restaurant;\n-- Creates the Customer table\nCREATE TABLE Customer (\n    custid INT PRIMARY KEY,\n    cust_firstname VARCHAR(50) NOT NULL,\n    cust_lastname VARCHAR(50) NOT NULL\n);\n-- Selects all values from the Customers table within the restaurant database\nSELECT * FROM restaurant.customer;\n-- Insert values into the Customer table\nINSERT INTO Customer (custid, cust_firstname, cust_lastname)\nVALUES\n    (1, 'John', 'Soda'),\n    (2, 'Jane', 'Smith'),\n    (3, 'Michael', 'Johnson'),\n    (4, 'Emily', 'Brown'),\n    (5, 'Robert', 'Wilson');\n-- Create the Cashier table\nCREATE TABLE Cashier (\n    cashier_id INT PRIMARY KEY,\n    cashier_name VARCHAR(50) NOT NULL,\n    hourly_wage DECIMAL(10, 2) NOT NULL\n);\n-- Create the Cashier table\nCREATE TABLE Cashier (\n    cashier_id INT PRIMARY KEY,\n    cashier_name VARCHAR(50) NOT NULL,\n    hourly_wage DECIMAL(10, 2) NOT NULL\n);\n-- Selects all values from the Cashier table within the restaurant database\nSELECT * FROM restaurant.cashier; \n-- Insert values into the Cashier table\nINSERT INTO Cashier (cashier_id, cashier_name, hourly_wage)\nVALUES\n    (1, 'Jay', 10),\n    (2, 'Samuel', 10.25),\n    (3, 'Noel', 11),\n    (4, 'Jill', 10.25),\n    (5, 'Philip', 12);\n-- Create the Address table\nCREATE TABLE Address (\n    address_id INT PRIMARY KEY,\n    delivery_address1 VARCHAR(200) NOT NULL,\n    delivery_address2 VARCHAR(200),\n    delivery_city VARCHAR(50) NOT NULL,\n    delivery_eircode VARCHAR(10) NOT NULL\n);\n-- Selects all values from the Address table within the restaurant database\nSELECT * FROM restaurant.address;\n-- Insert values into the Address table\nINSERT INTO Address (address_id, delivery_address1, delivery_address2, delivery_city, delivery_eircode)\nVALUES\n    (1, '123 Main Street', NULL, 'City A', 'EIR123'),\n    (2, '456 Elm Street', 'Apt 101', 'City B', 'EIR456'),\n    (3, '789 Oak Street', NULL, 'City C', 'EIR789'),\n    (4, '321 Pine Street', 'Suite 5', 'City D', 'EIR321'),\n    (5, '654 Birch Street', 'Apt 202', 'City E', 'EIR654');\n-- Create the Item table\nCREATE TABLE Item (\n    itemID INT PRIMARY KEY,\n    cashier_id INT NOT NULL,\n    custid INT NOT NULL,\n    address_id INT NOT NULL,\n    totalcost decimal(5, 2) NOT NULL,\n    FOREIGN KEY (cashier_id) REFERENCES Cashier(cashier_id),\n    FOREIGN KEY (custid) REFERENCES Customer(custid),\n    FOREIGN KEY (address_id) REFERENCES Address(address_id)\n);\n-- Selects all values from the Item table within the restaurant database\nSELECT * FROM restaurant.item;\n-- Insert values into the Item table\nINSERT INTO Item (itemID, cashier_id, custid, address_id, totalcost)\nVALUES\n    (1, 3, 1, 1, 25.50),\n    (2, 1, 2, 2, 30.75),\n    (3, 1, 3, 3, 18.90),\n    (4, 2, 4, 4, 42.25),\n    (5, 1, 5, 5, 22.60);\n-- Create the Mealone table\nCREATE TABLE Mealone (\n    itemID INT PRIMARY KEY,\n    Fruit_Cocktail VARCHAR(50),\n    FOREIGN KEY (itemID) REFERENCES Item(itemID)\n);\n-- Selects all values from the Mealone table within the restaurant database\nSELECT * FROM restaurant.mealone;\n-- Insert values into the Mealone table\nINSERT INTO Mealone (itemID, Fruit_Cocktail)\nVALUES\n    (1, 'Tropical Fruit Cocktail'),\n    (2, 'Mixed Berry Cocktail'),\n    (3, 'Pineapple and Mango Cocktail'),\n    (4, 'Exotic Fruit Delight'),\n    (5, 'Watermelon and Cantaloupe');\n-- Create the Mealtwo table\nCREATE TABLE Mealtwo (\n    itemID INT PRIMARY KEY,\n    Fruits VARCHAR(50),\n    FOREIGN KEY (itemID) REFERENCES Item(itemID)\n);\n-- Selects all values from the Mealtwo table within the restaurant database\nSELECT * FROM restaurant.mealtwo;\n-- Insert values into the Mealtwo table\nINSERT INTO Mealtwo (itemID, Fruits)\nVALUES\n    (1, 'Tropical Fruit Cocktail'),\n    (2, 'Strawberries and Kiwi'),\n    (3, 'Bananas and Grapes'),\n    (4, 'Watermelon and Cantaloupe'),\n    (5, 'Exotic Fruit Delight');\n-- Create the Mealthree table\nCREATE TABLE Mealthree (\n    itemID INT PRIMARY KEY,\n    Fruitsalad VARCHAR(50),\n    FOREIGN KEY (itemID) REFERENCES Item(itemID)\n);\n-- Selects all values from the Mealthree table within the restaurant database\nSELECT * FROM restaurant.mealthree;\n-- Insert values into the Mealthree table\nINSERT INTO Mealthree (itemID, Fruitsalad)\nVALUES\n    (1, 'Tropical Fruit Cocktail'),\n    (2, 'Tropical Fruitsalad'),\n    (3, 'Citrus Fruitsalad'),\n    (4, 'Citrus Fruit Medley'),\n    (5, 'Berry Fruitsalad');\n\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/index.html",
    "href": "code_projects/index.html",
    "title": "Code Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nePortfolio (this website)\n\n\n\n\n\n\nQuarto\n\n\nHTML\n\n\nSCSS\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nBoon Chin Look\n\n\n\n\n\n\n\n\n\n\n\n\nProduct Proposal\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nBoon Chin Look\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript Project: Interactive Fruit Shop Basket\n\n\n\n\n\n\nJavaScript\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nBoon Chin Look, Unknown\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Project - Restaurant Order Management Database\n\n\n\n\n\n\nSQL\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nBoon Chin Look\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "code_projects/eportfolio/index.html",
    "href": "code_projects/eportfolio/index.html",
    "title": "ePortfolio (this website)",
    "section": "",
    "text": "This project involved creating a professional ePortfolio using Quarto, Git, and GitHub, and deploying it through GitHub Pages. The goal was to showcase my technical skills, achievements, and professional development through an organized and interactive platform. Key sections include a landing page, CV, and code-based project highlights.\nThroughout this process, I honed my web development skills using Quarto, HTML, and SCSS, while also applying version control best practices with Git. A major focus was on multimedia integration, usability, and clean design principles to create a user-friendly and visually engaging site. I learned to step into the shoes of a first-time visitor, considering what they might be looking for or how they’d navigate the site, while also incorporating features and design elements that I personally enjoyed and felt reflected my style. This dual perspective helped me strike a balance between functionality and personal expression.\nThis project not only enhanced my technical skills but also served as a professional platform to present my development over time. It helped me reflect on my growth, from my starting point to my current capabilities and future ambitions. This live ePortfolio now stands as a reflection of my learning journey and ongoing professional growth. The full repository is accessible via the link above.\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boon Chin Look",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     CV\n  \n  \n    \n     Email\n  \n\n  \n  \n\nHi, nice to meet you!\n\n\nI’m a final-year Business Studies student specializing in Data Analytics.\n\nI thrive on problem‑solving, drawing data‑driven insights, and continuous learning.\n\nBeyond Academics:\n\nOutside the lecture hall, I’m passionate about rock climbing, fitness, and the arts, activities that push my boundaries mentally and physically.\nI’m also a keen cook and baker; experimenting in the kitchen fuels my creativity and deepens my appreciation for global cultures. Whether on a climbing wall or behind the stove, I love challenges that spark growth and curiosity.\n\n\n\n\n\n\n\nMy Skills:\n\nProgramming Languages: Python, HTML, CSS, JavaScript, SQL.\nSoftware & Tools: Microsoft Office, GitHub, Alteryx, Tableau, Power BI, Quarto, Jira.\n\n\n\nMusic I Love:\n\nClassical and ambient music helps me unwind while reading. Here are a few. Below are some pieces I typically listen to:\n\n\n  \n\nFind out more about my interests, community engagement and growth →\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/fruit_basket_project/index.html",
    "href": "code_projects/fruit_basket_project/index.html",
    "title": "JavaScript Project: Interactive Fruit Shop Basket",
    "section": "",
    "text": "This project was developed during an 8-week Introduction to JavaScript course offered by Girls Code First. Throughout the course, I built a solid foundation in JavaScript fundamentals while working collaboratively on a group project. Key topics covered included:\n\nConditional logical and functions.\nArrays and loops.\nJavaScript for websites: Document Object Model manipulation (Parts 1 & 2).\nProject planning and presentation\n\nFor our final project, my partner and I created an interactive fruit shop basket, completed within a 2-week deadline.\nThe app offers a simple and engaging shopping experience. When a user enters the site, they are greeted by name and prompted to enter an item they’d like to purchase. If the item is unavailable, a friendly apology message is shown. The application also dynamically calculates the contents and total of the user’s basket, enhancing interactivity and usability.\nThis project strengthened my understanding of DOM manipulation, user interaction, and the value of clear project planning under time constraints. It was also a great opportunity to collaborate and apply theory to a fun, practical outcome.\n\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/product_proposal/index.html",
    "href": "code_projects/product_proposal/index.html",
    "title": "Product Proposal",
    "section": "",
    "text": "This project encompassed both collaborative and individual components. As a team, we developed a product concept and identified a dataset relevant to our idea. The task involved pitching our application, designed specifically for students, focusing on finance, innovation, and enterprise. Our pitch was presented as a video, which I’ve embedded on the first page of the PDF above containing my individual report.\nIndividually, we were responsible for diving deeper into the technical side of our pitch. This included conducting data analysis, identifying the market gap, understanding target customers, and outlining a strategic approach to product sales. While teamwork was encouraged, each member was expected to make a unique and meaningful contribution in their final report.\nBelow are key snippets of code I developed during the data analysis phase, which played a critical role in shaping our product’s direction. I led this part of the project, as it closely aligned with my strengths. Using a preliminary dataset, I informed our pricing strategy, while an Uber Analysis Dataset enabled me to identify market gaps and opportunities. Below, I’ve included snippets from the latter dataset to demonstrate its role in our decision-making process.\n\n\nImporting Data and Intial Analysis\n\n\nImporting librarys\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\n\n\n\n\nImporting dataset\ndf = pd.read_csv(\"UberDataset.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\n\n\n\n\n0\n01/01/2016 21:11\n01/01/2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01/02/2016 01:25\n01/02/2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01/02/2016 20:25\n01/02/2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01/05/2016 17:31\n01/05/2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01/06/2016 14:42\n01/06/2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n\n\n\n\n\n\n\n\nLooking at data types of columns\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1155 entries, 0 to 1154\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   START_DATE  1155 non-null   object \n 1   END_DATE    1155 non-null   object \n 2   CATEGORY    1155 non-null   object \n 3   START       1155 non-null   object \n 4   STOP        1155 non-null   object \n 5   MILES       1155 non-null   float64\n 6   PURPOSE     653 non-null    object \ndtypes: float64(1), object(6)\nmemory usage: 63.3+ KB\n\n\n\n\nStandardize column names to lowercase for consistency\ndf.columns = df.columns.str.lower()\ndf.head()\n\n\n\n\n\n\n\n\nstart_date\nend_date\ncategory\nstart\nstop\nmiles\npurpose\n\n\n\n\n0\n01/01/2016 21:11\n01/01/2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01/02/2016 01:25\n01/02/2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01/02/2016 20:25\n01/02/2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01/05/2016 17:31\n01/05/2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01/06/2016 14:42\n01/06/2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n\n\n\n\n\n\n\n\nChecking for missing values in the dataset\ndf.isnull().sum()\n#no missing values found\n\n\nstart_date      0\nend_date        0\ncategory        0\nstart           0\nstop            0\nmiles           0\npurpose       502\ndtype: int64\n\n\n\n\nSimple descriptive statistics of each numerical column\ndf.describe()\n\n\n\n\n\n\n\n\nmiles\n\n\n\n\ncount\n1155.000000\n\n\nmean\n10.566840\n\n\nstd\n21.579106\n\n\nmin\n0.500000\n\n\n25%\n2.900000\n\n\n50%\n6.000000\n\n\n75%\n10.400000\n\n\nmax\n310.300000\n\n\n\n\n\n\n\n\nData Cleaning\n\n\nChecked & removed duplicates\nduplicated_data = df[df.duplicated()]\nprint(duplicated_data.shape)\n\ndf = df.drop_duplicates(keep='first')\n\n\n(1, 7)\n\n\n\n\nEncode categorical variables using one-hot encoding\ndata_encoded = pd.get_dummies(df, columns=['category', 'purpose'], drop_first=True)\n\n\n\n\nConverted date columns into date time\ndf[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors=\"coerce\")\ndf[\"end_date\"] = pd.to_datetime(df[\"end_date\"], errors=\"coerce\")\n\n\n\n\nHandling missing values\n# Handle missing values by dropping rows with missing 'start' or 'stop' and filling 'purpose' with 'unknown'\ndf.dropna(subset=['start', 'stop'], inplace=True)\ndf['purpose'] = df['purpose'].fillna(\"Unknown\") \n\n\n\n\nFeature engineering, created new column for trip date\ndf[\"date\"] = df[\"start_date\"].dt.date\n\n\n\n\nEnsured the dataset is sorted by date\ndf = df.sort_values('start_date')\ndf.set_index('start_date', inplace=True)\n\n\n\n\nAnalysis\n\n\nReturns first few rows of dataset to check\ndf.head()\n\n\n\n\n\n\n\n\nend_date\ncategory\nstart\nstop\nmiles\npurpose\ndate\n\n\nstart_date\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01 21:11:00\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n2016-01-01\n\n\n2016-01-02 01:25:00\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nUnknown\n2016-01-02\n\n\n2016-01-02 20:25:00\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n2016-01-02\n\n\n2016-01-05 17:31:00\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n2016-01-05\n\n\n2016-01-06 14:42:00\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n2016-01-06\n\n\n\n\n\n\n\n\nPlot trip frequency over time plot code\nplt.figure(figsize=(11, 7))\nsns.lineplot(x=df[\"date\"].value_counts().sort_index().index,\n             y=df[\"date\"].value_counts().sort_index().values, marker=\"o\")\nplt.xlabel(\"date\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Uber Trips Over Time\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTop 10 most common pickup and drop-off locations plot code\ntop_pickups = df[\"start\"].value_counts().head(10)\ntop_dropoffs = df[\"stop\"].value_counts().head(10)\n\nfig, axes = plt.subplots(1, 2, figsize=(9, 3))\n\nsns.barplot(x=top_pickups.values, y=top_pickups.index, ax=axes[0], palette=\"Blues_r\", hue=top_pickups.index, dodge=False)  \naxes[0].set_title(\"Top 10 Pickup Locations\")\naxes[0].set_xlabel(\"Number of Trips\")\naxes[0].legend([],[], frameon=False)  \nsns.barplot(x=top_dropoffs.values, y=top_dropoffs.index, ax=axes[1], palette=\"Greens_r\", hue=top_dropoffs.index, dodge=False)\naxes[1].set_title(\"Top 10 Drop-off Locations\")\naxes[1].set_xlabel(\"Number of Trips\")\naxes[1].legend([],[], frameon=False) \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode for plot of count of trips by purpose\ntrip_purpose_counts = df[\"purpose\"].value_counts()\nplt.figure(figsize=(9, 5))\nsns.barplot(x=trip_purpose_counts.index, y=trip_purpose_counts.values, hue=trip_purpose_counts.index, palette=\"Oranges\", legend=False) \nplt.xlabel(\"Purpose of Trip\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Trip Purpose Distribution\")\nplt.xticks(rotation=45)\nplt.show()\n\n#Main purpose isn't known but second is a meeting which could be relative to commuting with commuting having little, below we see it has the highest average distance by purpose, which backs up our product in relation a student carpooling application which would aid in their commute.\n\n\n\n\n\n\n\n\n\n\n\nCode for plot of average trip distance by purpose\navg_miles_purpose = df.groupby(\"purpose\")[\"miles\"].mean().sort_values()\nplt.figure(figsize=(9, 5))\nsns.barplot(x=avg_miles_purpose.index, y=avg_miles_purpose.values, hue=avg_miles_purpose.index, dodge=False, palette=\"Blues\", legend=False)  \nplt.xlabel(\"Purpose of Trip\")\nplt.ylabel(\"Average Miles Traveled\")\nplt.title(\"Average Trip Distance by Purpose\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPlot for hourly trip analysis of the day\ndf[\"hour\"] = df.index.hour\nplt.figure(figsize=(9, 5))\nsns.countplot(x=df[\"hour\"], hue=df[\"hour\"], dodge=False, palette=\"coolwarm\", legend=False) \nplt.xlabel(\"Hour of the Day\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Trips by Hour of the Day\")\nplt.show()\n#hours align similarly with college hours.\n\n\n\n\n\n\n\n\n\n\n\nCode for plot of Trip category to number of trips distribution\nplt.figure(figsize=(9, 5))\nsns.countplot(x=df[\"category\"], hue=df[\"category\"], palette=\"viridis\", legend=False)\nplt.xlabel(\"Trip Category\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Business vs Personal Trips\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Professional Development",
    "section": "",
    "text": "The following is a list of certificates and professional development courses I’ve completed:\n\nKubicleLinkedIn\n\n\n\n\n\n\n\n\n\n\nSubject\nCourse Name\nDate Completed\n\n\n\n\nAlteryx\nAlteryx Fundamentals\nApril 18th, 2024\n\n\nExcel\nMonte Carlo Simulation\nFebruary 21st, 2024\n\n\nExcel\nForward-Looking Models\nFebruary 10th, 2024\n\n\nExcel\nBuild Your First Model\nFebruary 7th, 2024\n\n\nExcel\nBuild Your First Dashboard\nFebruary 4th, 2024\n\n\nMachine Learning with Python\nAdvanced Clustering\nJanuary 22nd, 2024\n\n\nMachine Learning with Python\nAdvanced Classification\nJanuary 20th, 2024\n\n\nMachine Learning with Python\nAdvanced Regression\nJanuary 19th, 2024\n\n\nMachine Learning with Python\nFeature Engineering\nJanuary 16th, 2024\n\n\nMachine Learning with Python\nK-Means Clustering in Python\nJanuary 12th, 2024\n\n\nMachine Learning with Python\nDecision Trees in Python\nJanuary 12th, 2024\n\n\nMachine Learning with Python\nRegression Analysis in Python\nJanuary 9th, 2024\n\n\nPower BI\nAdvanced DAX Functions and Concepts\nJanuary 8th, 2024\n\n\nData Presentation Skills\nPresenting Your Data\nJanuary 1st, 2024\n\n\nData Presentation Skills\nCommunicating Data Effectively\nDecember 30th, 2023\n\n\nData Presentation Skills\nTelling Stories with Data\nDecember 29th, 2023\n\n\nPower BI\nIntroduction to DAX\nDecember 25th, 2023\n\n\nPower BI\nThe Query Editor\nDecember 23rd, 2023\n\n\nPower BI\nDesigning Effective Dashboards\nDecember 21st, 2023\n\n\nPower BI\nDeveloping Effective Visualizations\nDecember 21st, 2023\n\n\nPower BI\nIntroduction to Visualizations in Power BI\nDecember 17th, 2023\n\n\nVisualization Fundamentals\nApplying Visual Data Skills\nDecember 17th, 2023\n\n\nData Strategy & Governance\nBuilding a Data-Driven Culture\nDecember 14th, 2023\n\n\nData Strategy & Governance\nDefining Your Data Strategy\nDecember 13th, 2023\n\n\nData Security\nHandling Data Securely and Responsibly\nNovember 28th, 2023\n\n\nData Security\nKeeping Your Information Safe\nNovember 27th, 2023\n\n\nData Ethics & Risk\nApplying Ethical Thinking\nNovember 23rd, 2023\n\n\nData Ethics & Risk\nIntroduction to Data Ethics\nNovember 17th, 2023\n\n\nAI Fundamentals\nLarge Language Models\nNovember 16th, 2023\n\n\nAI Fundamentals\nGenerative AI and ChatGPT\nNovember 15th, 2023\n\n\nAI Fundamentals\nIdentifying Patterns\nNovember 14th, 2023\n\n\nAI Fundamentals\nUnderstanding Natural Language\nNovember 13th, 2023\n\n\nAI Fundamentals\nPredicting Scenarios\nNovember 6th, 2023\n\n\nAI Fundamentals\nPredicting Future Values\nOctober 26th, 2023\n\n\nAI Fundamentals\nBusiness Applications of AI\nOctober 16th, 2023\n\n\nFinancial Modeling\nIntroduction to Financial Statements\nOctober 15th, 2023\n\n\nAI Fundamentals\nIntroduction to Artificial Intelligence\nOctober 13th, 2023\n\n\nStatistical Analysis\nHypothesis Testing\nOctober 10th, 2023\n\n\nStatistical Analysis\nProbability Distributions\nOctober 5th, 2023\n\n\nStatistical Analysis\nProbability Principles\nOctober 3rd, 2023\n\n\nStatistical Analysis\nIntroduction to Predictive Modeling\nOctober 1st, 2023\n\n\nPython Fundamentals\nConnecting to Live Data\nSeptember 30th, 2023\n\n\nPython Fundamentals\nData Preparation\nSeptember 25th, 2023\n\n\nPython Fundamentals\nStoring, Transforming and Visualizing Data\nSeptember 21st, 2023\n\n\nPython Fundamentals\nFunctions, Conditionality and Loops\nSeptember 17th, 2023\n\n\nPower BI\nBuilding Your First Dashboard\nMay 1st, 2023\n\n\nSQL\nData Manipulation with SQL\nApril 30th, 2023\n\n\nSQL\nAggregating Data with SQL\nFebruary 6th, 2023\n\n\nTableau\nIntroducing Tableau\nFebruary 1st, 2023\n\n\nSQL\nJoining Data with SQL\nJanuary 7th, 2023\n\n\nSQL\nSelecting and Filtering Data with SQL\nJanuary 4th, 2023\n\n\nSQL\nUnderstanding SQL Databases\nJanuary 2nd, 2023\n\n\nData Literacy\nThinking and Communicating with Data\nDecember 26th, 2022\n\n\nExcel\nIntroduction to Excel Macros\nDecember 26th, 2022\n\n\nExcel\nProject: Optimize company revenue with cost analysis\nDecember 25th, 2022\n\n\nExcel\nFinance Functions\nDecember 24th, 2022\n\n\nExcel\nText, Time, and Dates\nDecember 10th, 2022\n\n\nPython Fundamentals\nPython Fundamentals\nDecember 7th, 2022\n\n\nVisualization Fundamentals\nVisual Data Thinking\nDecember 4th, 2022\n\n\nExcel\nLookups and Database Functions\nNovember 24th, 2022\n\n\nExcel\nCharts in Depth\nNovember 21st, 2022\n\n\nStatistical Analysis\nPrinciples of Statistics\nNovember 18th, 2022\n\n\nExcel\nPivot Tables\nNovember 3rd, 2022\n\n\nData Literacy\nIntroduction to Data Preparation\nOctober 26th, 2022\n\n\nExcel\nFormulas and Functions\nOctober 26th, 2022\n\n\nExcel\nData Manipulation and Formatting\nOctober 8th, 2022\n\n\nData Literacy\nIntroduction to Data and Databases\nOctober 7th, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nCourse Name\nDate Completed\n\n\n\n\nSocial Media Marketing\nSocial Media Marketing Strategy: TikTok and Instagram Reels\nFebruary 5th, 2024\n\n\nInstagram Marketing\nMarketing on Instagram (2021)\nFebruary 3th, 2024\n\n\nSocial Media Marketing\nSocial Media Marketing with Facebook and Twitter\nFebruary 2nd, 2024\n\n\nData Science Foundations\nData Science Foundations: Fundamentals (2022)\nJanuary 31st, 2024\n\n\nData Analytics\nData Analytics for Business Professionals (2022)\nJanuary 28th, 2024\n\n\nBig Data\nBig Data in the Age of AI (2019)\nJanuary 28th, 2024\n\n\nBusiness Analytics\nIntroduction to Business Analytics\nJanuary 24th, 2024\n\n\nSEO\nSEO Foundations\nJanuary 23th, 2024\n\n\n\n\n\n\nCheck out my Experiences & Growth page to explore my certifications, involvements and interests! →"
  },
  {
    "objectID": "courses.html#course-certificates",
    "href": "courses.html#course-certificates",
    "title": "Professional Development",
    "section": "",
    "text": "The following is a list of certificates and professional development courses I’ve completed:\n\nKubicleLinkedIn\n\n\n\n\n\n\n\n\n\n\nSubject\nCourse Name\nDate Completed\n\n\n\n\nAlteryx\nAlteryx Fundamentals\nApril 18th, 2024\n\n\nExcel\nMonte Carlo Simulation\nFebruary 21st, 2024\n\n\nExcel\nForward-Looking Models\nFebruary 10th, 2024\n\n\nExcel\nBuild Your First Model\nFebruary 7th, 2024\n\n\nExcel\nBuild Your First Dashboard\nFebruary 4th, 2024\n\n\nMachine Learning with Python\nAdvanced Clustering\nJanuary 22nd, 2024\n\n\nMachine Learning with Python\nAdvanced Classification\nJanuary 20th, 2024\n\n\nMachine Learning with Python\nAdvanced Regression\nJanuary 19th, 2024\n\n\nMachine Learning with Python\nFeature Engineering\nJanuary 16th, 2024\n\n\nMachine Learning with Python\nK-Means Clustering in Python\nJanuary 12th, 2024\n\n\nMachine Learning with Python\nDecision Trees in Python\nJanuary 12th, 2024\n\n\nMachine Learning with Python\nRegression Analysis in Python\nJanuary 9th, 2024\n\n\nPower BI\nAdvanced DAX Functions and Concepts\nJanuary 8th, 2024\n\n\nData Presentation Skills\nPresenting Your Data\nJanuary 1st, 2024\n\n\nData Presentation Skills\nCommunicating Data Effectively\nDecember 30th, 2023\n\n\nData Presentation Skills\nTelling Stories with Data\nDecember 29th, 2023\n\n\nPower BI\nIntroduction to DAX\nDecember 25th, 2023\n\n\nPower BI\nThe Query Editor\nDecember 23rd, 2023\n\n\nPower BI\nDesigning Effective Dashboards\nDecember 21st, 2023\n\n\nPower BI\nDeveloping Effective Visualizations\nDecember 21st, 2023\n\n\nPower BI\nIntroduction to Visualizations in Power BI\nDecember 17th, 2023\n\n\nVisualization Fundamentals\nApplying Visual Data Skills\nDecember 17th, 2023\n\n\nData Strategy & Governance\nBuilding a Data-Driven Culture\nDecember 14th, 2023\n\n\nData Strategy & Governance\nDefining Your Data Strategy\nDecember 13th, 2023\n\n\nData Security\nHandling Data Securely and Responsibly\nNovember 28th, 2023\n\n\nData Security\nKeeping Your Information Safe\nNovember 27th, 2023\n\n\nData Ethics & Risk\nApplying Ethical Thinking\nNovember 23rd, 2023\n\n\nData Ethics & Risk\nIntroduction to Data Ethics\nNovember 17th, 2023\n\n\nAI Fundamentals\nLarge Language Models\nNovember 16th, 2023\n\n\nAI Fundamentals\nGenerative AI and ChatGPT\nNovember 15th, 2023\n\n\nAI Fundamentals\nIdentifying Patterns\nNovember 14th, 2023\n\n\nAI Fundamentals\nUnderstanding Natural Language\nNovember 13th, 2023\n\n\nAI Fundamentals\nPredicting Scenarios\nNovember 6th, 2023\n\n\nAI Fundamentals\nPredicting Future Values\nOctober 26th, 2023\n\n\nAI Fundamentals\nBusiness Applications of AI\nOctober 16th, 2023\n\n\nFinancial Modeling\nIntroduction to Financial Statements\nOctober 15th, 2023\n\n\nAI Fundamentals\nIntroduction to Artificial Intelligence\nOctober 13th, 2023\n\n\nStatistical Analysis\nHypothesis Testing\nOctober 10th, 2023\n\n\nStatistical Analysis\nProbability Distributions\nOctober 5th, 2023\n\n\nStatistical Analysis\nProbability Principles\nOctober 3rd, 2023\n\n\nStatistical Analysis\nIntroduction to Predictive Modeling\nOctober 1st, 2023\n\n\nPython Fundamentals\nConnecting to Live Data\nSeptember 30th, 2023\n\n\nPython Fundamentals\nData Preparation\nSeptember 25th, 2023\n\n\nPython Fundamentals\nStoring, Transforming and Visualizing Data\nSeptember 21st, 2023\n\n\nPython Fundamentals\nFunctions, Conditionality and Loops\nSeptember 17th, 2023\n\n\nPower BI\nBuilding Your First Dashboard\nMay 1st, 2023\n\n\nSQL\nData Manipulation with SQL\nApril 30th, 2023\n\n\nSQL\nAggregating Data with SQL\nFebruary 6th, 2023\n\n\nTableau\nIntroducing Tableau\nFebruary 1st, 2023\n\n\nSQL\nJoining Data with SQL\nJanuary 7th, 2023\n\n\nSQL\nSelecting and Filtering Data with SQL\nJanuary 4th, 2023\n\n\nSQL\nUnderstanding SQL Databases\nJanuary 2nd, 2023\n\n\nData Literacy\nThinking and Communicating with Data\nDecember 26th, 2022\n\n\nExcel\nIntroduction to Excel Macros\nDecember 26th, 2022\n\n\nExcel\nProject: Optimize company revenue with cost analysis\nDecember 25th, 2022\n\n\nExcel\nFinance Functions\nDecember 24th, 2022\n\n\nExcel\nText, Time, and Dates\nDecember 10th, 2022\n\n\nPython Fundamentals\nPython Fundamentals\nDecember 7th, 2022\n\n\nVisualization Fundamentals\nVisual Data Thinking\nDecember 4th, 2022\n\n\nExcel\nLookups and Database Functions\nNovember 24th, 2022\n\n\nExcel\nCharts in Depth\nNovember 21st, 2022\n\n\nStatistical Analysis\nPrinciples of Statistics\nNovember 18th, 2022\n\n\nExcel\nPivot Tables\nNovember 3rd, 2022\n\n\nData Literacy\nIntroduction to Data Preparation\nOctober 26th, 2022\n\n\nExcel\nFormulas and Functions\nOctober 26th, 2022\n\n\nExcel\nData Manipulation and Formatting\nOctober 8th, 2022\n\n\nData Literacy\nIntroduction to Data and Databases\nOctober 7th, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nCourse Name\nDate Completed\n\n\n\n\nSocial Media Marketing\nSocial Media Marketing Strategy: TikTok and Instagram Reels\nFebruary 5th, 2024\n\n\nInstagram Marketing\nMarketing on Instagram (2021)\nFebruary 3th, 2024\n\n\nSocial Media Marketing\nSocial Media Marketing with Facebook and Twitter\nFebruary 2nd, 2024\n\n\nData Science Foundations\nData Science Foundations: Fundamentals (2022)\nJanuary 31st, 2024\n\n\nData Analytics\nData Analytics for Business Professionals (2022)\nJanuary 28th, 2024\n\n\nBig Data\nBig Data in the Age of AI (2019)\nJanuary 28th, 2024\n\n\nBusiness Analytics\nIntroduction to Business Analytics\nJanuary 24th, 2024\n\n\nSEO\nSEO Foundations\nJanuary 23th, 2024\n\n\n\n\n\n\nCheck out my Experiences & Growth page to explore my certifications, involvements and interests! →"
  },
  {
    "objectID": "future_goals.html",
    "href": "future_goals.html",
    "title": "Future Pathway: Pursuing a Career in Technology",
    "section": "",
    "text": "With a foundation in Business with Data Analytics and a growing passion for technology, my future is shaped by both strategic intent and hands-on experience. From early exposure to software development through mentorship with Innovative Communities, to gaining practical insights during the Dell Technologies STEM Aspire programme, I’ve developed a clear vision for building a career in the tech sector.\nMotivated by the rapidly evolving technological landscape, I intentionally chose Data Analytics as my specialization within Business Studies, completing modules such as Programming & Visualization, Machine Learning & Advanced Python, Workflow & Data Management, Econometrics, and Forecasting. Beyond university, I’ve also completed external courses in software development, which have further strengthened my technical foundation and reaffirmed my ambition to become a software developer.\nMy professional growth is also reflected in my academic projects, most notably, the development of a Quarto-based ePortfolio hosted on GitHub Pages (this website). This project sharpened my skills in web development and version control while providing a structured and industry-relevant way to showcase my journey and capabilities. It gave me valuable insight into my progress; where I started, where I am now, and where I aspire to be in the future.\nLooking ahead, I plan to develop more code-based projects and pursue courses across the tech space as part of my continuing professional development (CPD), further bridging the gap between my business acumen and technical skill set.\nUltimately, I am driven by a commitment to continuous learning, adaptability, and a passion for solving complex problems. My goal is to thrive in a dynamic tech environment where I can contribute meaningfully to a company and continue to grow both personally and professionally.\n\nFind out more about my interests, community engagement and growth →\nExplore the short courses I’ve completed →\n\n\n Back to top"
  },
  {
    "objectID": "reports/ethical_theory_creation/index.html",
    "href": "reports/ethical_theory_creation/index.html",
    "title": "Creating an Ethical Theory",
    "section": "",
    "text": "As part of a Business and Professional Ethics module, we were assigned a paired project that challenged us to reflect deeply on ethical theories discussed in class, including those of Aristotle, Kant, and Bentham. Our objective was to design a unique ethical framework that answers the fundamental question: “Why do we act?”\nThrough collaborative dialogue and thoughtful exchange of ideas, my partner and I explored philosophical principles to craft a coherent and creative ethical theory. This process involved naming our theory, defining its core inspiration, critically comparing it with existing frameworks, and demonstrating its application in a real-world scenario.\nDeep thinking, creativity, and teamwork were central to this assignment, culminating in the development of our theory: the Unified Fulfilment Theory (UFT).\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/modernization&dependecy_theories/index.html",
    "href": "reports/modernization&dependecy_theories/index.html",
    "title": "Analysis Comparison Modernization and Dependency Theories",
    "section": "",
    "text": "This project critically explored the contrasting perspectives of the Stages of Growth (“modernization”) and Dependency schools of thought on economic development. It examined the historical, social, and political contexts that shaped each theory, highlighting how these foundations informed their differing interpretations of development processes. The analysis also delved into their core arguments and policy recommendations, drawing attention to the unique implications each perspective held for global economic strategy.\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/technological&consumer_changes_in_autombile_industry/index.html",
    "href": "reports/technological&consumer_changes_in_autombile_industry/index.html",
    "title": "Analysis of the Impact of Technological Change and Shifting Consumer Behaviour on the Global Automobile Industry",
    "section": "",
    "text": "This assignment explored how advancements in technology and shifting consumer preferences have shaped the structure, strategies, and market dynamics of the global automobile industry. It deepened my understanding of how external forces drive transformation in established sectors and sharpened my analytical thinking around industry adaptation.\n\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/product_proposal/preliminary_analysis.html",
    "href": "code_projects/product_proposal/preliminary_analysis.html",
    "title": "Importing data and intial analysis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\nimport statsmodels.api as sm\n\n\n\n\ndf = pd.read_csv('/content/preliminary_analysis.csv')\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\ntrip_distance_km\ntrip_duration_min\nfare_amount_euro\ntime_of_day\nday_of_week\ndriver_rating\npayment_method\ntraffic_condition\nweather_condition\nservice_fee_to_app\npassenger_count\nfuel_cost_per_km\n\n\n\n\n0\n11.968397\n4.305691\n30.778773\nevening\nSunday\n3.1\ncash\nheavy\nrain\n4.818550\n4\n1.196840\n\n\n1\n7.472324\n15.616635\n28.018259\nafternoon\nSaturday\n4.4\ncash\nlight\nfog\n1.514309\n4\n0.747232\n\n\n2\n6.911418\n17.753774\n15.629470\nafternoon\nThursday\n4.2\ncash\nmoderate\nclear\n3.789845\n3\n0.691142\n\n\n3\n6.911511\n4.042930\n16.571829\nmorning\nFriday\n4.9\napp\nmoderate\nrain\n3.024924\n3\n0.691151\n\n\n4\n23.248572\n12.985014\n18.462118\nafternoon\nWednesday\n3.5\ncard\nheavy\nrain\n1.102529\n1\n2.324857\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ndf.info() #check if anything is missing in the data\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1003 entries, 0 to 1002\nData columns (total 12 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   trip_distance_km    1003 non-null   float64\n 1   trip_duration_min   1003 non-null   float64\n 2   fare_amount_euro    1003 non-null   float64\n 3   time_of_day         1003 non-null   object \n 4   day_of_week         1003 non-null   object \n 5   driver_rating       1003 non-null   float64\n 6   payment_method      1003 non-null   object \n 7   traffic_condition   1003 non-null   object \n 8   weather_condition   1003 non-null   object \n 9   service_fee_to_app  1003 non-null   float64\n 10  passenger_count     1003 non-null   int64  \n 11  fuel_cost_per_km    1003 non-null   float64\ndtypes: float64(6), int64(1), object(5)\nmemory usage: 94.2+ KB\n\n\n\ndf.describe() #gives quick descriptive statistics of each numerical column\n#min distance is 0.2km, max 38km\n#mean fare 25 euro, min 4, max 82\n#service fee to app mean fee 3, max 5, min 1\n\n\n  \n    \n\n\n\n\n\n\ntrip_distance_km\ntrip_duration_min\nfare_amount_euro\ndriver_rating\nservice_fee_to_app\npassenger_count\nfuel_cost_per_km\n\n\n\n\ncount\n1003.000000\n1003.000000\n1003.000000\n1003.00000\n1003.000000\n1003.000000\n1003.000000\n\n\nmean\n10.315106\n20.328322\n25.931628\n3.99332\n3.013213\n2.464606\n1.031511\n\n\nstd\n7.016247\n14.413322\n10.665800\n0.59168\n1.158138\n1.103204\n0.701625\n\n\nmin\n0.229595\n0.258605\n4.691645\n3.00000\n1.009818\n1.000000\n0.022959\n\n\n25%\n5.136928\n9.825174\n18.351567\n3.50000\n2.034586\n1.000000\n0.513693\n\n\n50%\n8.650479\n17.267642\n24.773245\n4.00000\n2.983360\n2.000000\n0.865048\n\n\n75%\n13.789172\n26.640214\n31.670482\n4.50000\n4.026201\n3.000000\n1.378917\n\n\nmax\n38.934452\n136.102327\n82.983566\n5.00000\n4.998693\n4.000000\n3.893445\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nData Cleaning\n\n#Removed duplicates\nduplicated_data = df[df.duplicated()]\nprint(duplicated_data.shape)\n\ndf = df.drop_duplicates(keep='first')\n\n(0, 12)\n\n\n\n# Standardize column names to lowercase\ndf.columns = df.columns.str.lower()\ndf\n#cost per km instead of fuel cost per km\n\n\n  \n    \n\n\n\n\n\n\ntrip_distance_km\ntrip_duration_min\nfare_amount_euro\ntime_of_day\nday_of_week\ndriver_rating\npayment_method\ntraffic_condition\nweather_condition\nservice_fee_to_app\npassenger_count\nfuel_cost_per_km\n\n\n\n\n0\n11.968397\n4.305691\n30.778773\nevening\nSunday\n3.1\ncash\nheavy\nrain\n4.818550\n4\n1.196840\n\n\n1\n7.472324\n15.616635\n28.018259\nafternoon\nSaturday\n4.4\ncash\nlight\nfog\n1.514309\n4\n0.747232\n\n\n2\n6.911418\n17.753774\n15.629470\nafternoon\nThursday\n4.2\ncash\nmoderate\nclear\n3.789845\n3\n0.691142\n\n\n3\n6.911511\n4.042930\n16.571829\nmorning\nFriday\n4.9\napp\nmoderate\nrain\n3.024924\n3\n0.691151\n\n\n4\n23.248572\n12.985014\n18.462118\nafternoon\nWednesday\n3.5\ncard\nheavy\nrain\n1.102529\n1\n2.324857\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n998\n5.858017\n17.109830\n14.965427\nafternoon\nThursday\n4.9\napp\nlight\nrain\n4.688525\n3\n0.585802\n\n\n999\n4.615233\n18.146223\n31.758533\nafternoon\nFriday\n4.6\ncard\nmoderate\nsnow\n3.626891\n1\n0.461523\n\n\n1000\n15.200000\n20.500000\n25.000000\nmorning\nMonday\n4.5\ncard\nlight\nclear\n2.500000\n2\n1.520000\n\n\n1001\n8.700000\n12.300000\n18.000000\nevening\nTuesday\n4.0\napp\nmoderate\nrain\n1.800000\n1\n0.870000\n\n\n1002\n22.500000\n35.000000\n35.000000\nafternoon\nWednesday\n4.8\ncash\nheavy\nfog\n3.500000\n3\n2.250000\n\n\n\n\n1003 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\nAnalysis\n\nimport plotly.express as px\nimport calendar\n\n# Group data by day of the week and calculate total fare\ndaily_fare = df.groupby('day_of_week')['fare_amount_euro'].sum().reset_index()\n\n# Get ordered list of weekdays\nweekdays = list(calendar.day_name)\n\n# Create the bar chart with specified order\nfig = px.bar(daily_fare,\n             x='day_of_week',\n             y='fare_amount_euro',\n             title='Total Fare Amount by Day of Week',\n             labels={'fare_amount_euro': 'Total Fare (Euro)'},\n             category_orders={\"day_of_week\": weekdays}) # Ensures days are ordered\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# prompt: can you create a barchart for the dataset on time_of_day\n\n# Assuming 'time_of_day' is a column in your DataFrame 'df'\ntime_of_day_counts = df['time_of_day'].value_counts()\n\nplt.figure(figsize=(10, 6))\nplt.bar(time_of_day_counts.index, time_of_day_counts.values)\nplt.xlabel('Time of Day')\nplt.ylabel('Number of Rides')\nplt.title('Number of Rides by Time of Day')\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf['trip_distance_km'].mean().round(2)\n\nnp.float64(10.32)\n\n\n\ndf['fare_amount_euro'].mean().round(2)\n\n\nnp.float64(25.93)\n\n\n\n# Outlier detection using IQR\nQ1 = df['fare_amount_euro'].quantile(0.25)\nQ3 = df['fare_amount_euro'].quantile(0.75)\nIQR = Q3 - Q1\n\noutliers = df[(df['fare_amount_euro'] &lt; (Q1 - 1.5 * IQR)) | (df['fare_amount_euro'] &gt; (Q3 + 1.5 * IQR))]\noutliers\n\n\n  \n    \n\n\n\n\n\n\ntrip_distance_km\ntrip_duration_min\nfare_amount_euro\ntime_of_day\nday_of_week\ndriver_rating\npayment_method\ntraffic_condition\nweather_condition\nservice_fee_to_app\npassenger_count\nfuel_cost_per_km\n\n\n\n\n38\n5.607175\n31.571824\n65.534792\nevening\nFriday\n3.2\napp\nlight\nrain\n4.798150\n4\n0.560717\n\n\n53\n10.640638\n10.128370\n81.927954\nafternoon\nTuesday\n4.4\ncard\nmoderate\nsnow\n2.216440\n1\n1.064064\n\n\n77\n5.186335\n56.887244\n57.781359\nevening\nWednesday\n4.8\ncard\nmoderate\nrain\n2.249624\n1\n0.518634\n\n\n158\n23.935761\n16.673484\n52.262151\nafternoon\nFriday\n3.1\ncash\nmoderate\nfog\n3.770454\n2\n2.393576\n\n\n166\n23.335719\n26.907486\n59.280678\nevening\nWednesday\n4.1\napp\nmoderate\nsnow\n4.480221\n3\n2.333572\n\n\n253\n23.886286\n14.342414\n52.298031\nmorning\nMonday\n4.5\ncard\nheavy\nsnow\n3.124438\n4\n2.388629\n\n\n336\n13.121622\n19.488230\n52.124291\nafternoon\nFriday\n4.0\napp\nlight\nrain\n3.650890\n1\n1.312162\n\n\n395\n6.140473\n38.452705\n54.048972\nmorning\nMonday\n4.7\napp\nmoderate\nfog\n3.671735\n2\n0.614047\n\n\n414\n9.485639\n71.725144\n63.765245\nnight\nSaturday\n3.7\ncard\nheavy\nclear\n3.116413\n3\n0.948564\n\n\n460\n19.351462\n42.311928\n60.631101\nnight\nMonday\n3.5\ncard\nheavy\nrain\n4.863166\n3\n1.935146\n\n\n570\n6.647348\n49.766208\n60.872633\nnight\nWednesday\n3.4\ncard\nlight\nrain\n3.610108\n1\n0.664735\n\n\n604\n10.131767\n136.102327\n82.983566\nevening\nSunday\n4.9\napp\nmoderate\nsnow\n1.400845\n4\n1.013177\n\n\n642\n23.949070\n6.075658\n75.356599\nafternoon\nMonday\n4.0\ncard\nheavy\nfog\n3.688338\n1\n2.394907\n\n\n691\n13.130317\n9.083073\n78.319730\nmorning\nFriday\n4.8\ncard\nlight\nrain\n2.526298\n4\n1.313032\n\n\n758\n1.953581\n32.694838\n58.336350\nafternoon\nWednesday\n4.9\napp\nmoderate\nrain\n1.435887\n1\n0.195358\n\n\n775\n4.717621\n23.196998\n55.235470\nmorning\nWednesday\n3.3\napp\nheavy\nrain\n1.522939\n1\n0.471762\n\n\n782\n23.022863\n20.604813\n67.580561\nevening\nMonday\n4.0\ncard\nmoderate\nrain\n2.510182\n3\n2.302286\n\n\n783\n11.669024\n6.889679\n56.016481\nmorning\nSaturday\n4.9\ncard\nmoderate\nclear\n4.154746\n4\n1.166902\n\n\n810\n7.300447\n30.831632\n61.486360\nmorning\nSunday\n3.7\ncard\nheavy\nfog\n3.862706\n1\n0.730045\n\n\n848\n4.089061\n13.725879\n53.822120\nafternoon\nSaturday\n4.2\napp\nmoderate\nfog\n2.346823\n2\n0.408906\n\n\n885\n14.583592\n22.952440\n53.239202\nnight\nTuesday\n4.1\ncard\nlight\nfog\n4.986935\n1\n1.458359\n\n\n905\n23.835042\n15.268144\n60.103410\nevening\nWednesday\n4.4\ncard\nheavy\nfog\n3.980531\n2\n2.383504\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Encoding categorical variables\ndf = pd.get_dummies(df, columns=[\"time_of_day\", \"day_of_week\", \"payment_method\", \"traffic_condition\", \"weather_condition\"], drop_first=True)\ndf\n\n\n  \n    \n\n\n\n\n\n\ntrip_distance_km\ntrip_duration_min\nfare_amount_euro\ndriver_rating\nservice_fee_to_app\npassenger_count\nfuel_cost_per_km\ntime_of_day_evening\ntime_of_day_morning\ntime_of_day_night\n...\nday_of_week_Thursday\nday_of_week_Tuesday\nday_of_week_Wednesday\npayment_method_card\npayment_method_cash\ntraffic_condition_light\ntraffic_condition_moderate\nweather_condition_fog\nweather_condition_rain\nweather_condition_snow\n\n\n\n\n0\n11.968397\n4.305691\n30.778773\n3.1\n4.818550\n4\n1.196840\nTrue\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n1\n7.472324\n15.616635\n28.018259\n4.4\n1.514309\n4\n0.747232\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n2\n6.911418\n17.753774\n15.629470\n4.2\n3.789845\n3\n0.691142\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n3\n6.911511\n4.042930\n16.571829\n4.9\n3.024924\n3\n0.691151\nFalse\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n23.248572\n12.985014\n18.462118\n3.5\n1.102529\n1\n2.324857\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n998\n5.858017\n17.109830\n14.965427\n4.9\n4.688525\n3\n0.585802\nFalse\nFalse\nFalse\n...\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n999\n4.615233\n18.146223\n31.758533\n4.6\n3.626891\n1\n0.461523\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n1000\n15.200000\n20.500000\n25.000000\n4.5\n2.500000\n2\n1.520000\nFalse\nTrue\nFalse\n...\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1001\n8.700000\n12.300000\n18.000000\n4.0\n1.800000\n1\n0.870000\nTrue\nFalse\nFalse\n...\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n1002\n22.500000\n35.000000\n35.000000\n4.8\n3.500000\n3\n2.250000\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n1003 rows × 23 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nprint(df['time_of_day_evening'.lower()].corr(df['fare_amount_euro']))\nprint(df['time_of_day_morning'.lower()].corr(df['fare_amount_euro']))\nprint(df['time_of_day_night'.lower()].corr(df['fare_amount_euro']))\n\nprint(df['day_of_week_Monday'].corr(df['fare_amount_euro']))\nprint(df['day_of_week_Tuesday'].corr(df['fare_amount_euro']))\nprint(df['day_of_week_Wednesday'].corr(df['fare_amount_euro']))\nprint(df['day_of_week_Sunday'].corr(df['fare_amount_euro']))\n\n0.0031032086872480806\n-0.006831352364888633\n0.04068203159916684\n0.015887074995456146\n-0.0015255845279042036\n0.05284823573435124\n-0.011212825560587933\n\n\n\n# Box plot\nbox_plot = px.box(df, y='fare_amount_euro', title=\"Fare Amount Distribution (with Outliers)\")\nbox_plot\n\n\n\n\n                                \n                                            \n\n\n\n\n\nnumerical_columns = df.select_dtypes(include=np.number).columns.tolist()\nprint(numerical_columns)\n\n['trip_distance_km', 'trip_duration_min', 'fare_amount_euro', 'driver_rating', 'service_fee_to_app', 'passenger_count', 'fuel_cost_per_km']\n\n\n\n# Correlation scatter plot\nfor column in numerical_columns:\n    px.scatter(df, x='fare_amount_euro', y=column, trendline=\"ols\").show()\n#as distance increases - cost increases\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# OLS Regression\nX = df[['trip_distance_km', 'trip_duration_min', 'service_fee_to_app', 'passenger_count']]\ny = df['fare_amount_euro']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n#R-squared = 21.3% This means that only 21.3% of the variation in fare_amount_euro (the price of the ride) is explained by the independent variables (distance, duration, driver rating, service fee, and passenger count).\n#Adjusted r-squared = This is similar to R-squared but adjusted for the number of predictors. It suggests the model does not improve significantly with more variables.\n#Skew = 1.424, Kurtosis = 6.470 → Suggests the model has some skewed residuals and heavy tails, meaning extreme values may be affecting predictions.\n#The model as a whole is statistically significant (since the p-value is extremely low), meaning at least one predictor significantly affects fare_amount_euro.\n\n#Trip Distance (€0.51 per km, p = 0.000) → For every additional kilometer traveled, the fare increases by €0.51 - significant\n#Trip Duration (€0.20 per minute, p = 0.000) → For every extra minute of travel, the fare increases by €0.20. - significant.\n#Service Fee (€1.11 per unit, p = 0.000) → A higher service fee strongly increases fare. - highly significant.\n#Passenger Count and driver rating does not have effect on fares\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:       fare_amount_euro   R-squared:                       0.211\nModel:                            OLS   Adj. R-squared:                  0.208\nMethod:                 Least Squares   F-statistic:                     66.70\nDate:                Thu, 10 Apr 2025   Prob (F-statistic):           4.81e-50\nTime:                        17:23:05   Log-Likelihood:                -3678.0\nNo. Observations:                1003   AIC:                             7366.\nDf Residuals:                     998   BIC:                             7391.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nconst                 13.2025      1.210     10.914      0.000      10.829      15.576\ntrip_distance_km       0.5090      0.043     11.888      0.000       0.425       0.593\ntrip_duration_min      0.2013      0.021      9.667      0.000       0.160       0.242\nservice_fee_to_app     1.1300      0.259      4.358      0.000       0.621       1.639\npassenger_count       -0.0070      0.272     -0.026      0.979      -0.541       0.527\n==============================================================================\nOmnibus:                      289.864   Durbin-Watson:                   1.984\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              865.910\nSkew:                           1.436   Prob(JB):                    9.33e-189\nKurtosis:                       6.531   Cond. No.                         110.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Remove outliers based on IQR for 'fare_amount_euro' from OLS test\nQ1 = df['fare_amount_euro'].quantile(0.25)\nQ3 = df['fare_amount_euro'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Filter out the rows that are outliers\ndf_outliers_removed = df[(df['fare_amount_euro'] &gt;= (Q1 - 1.5 * IQR)) & (df['fare_amount_euro'] &lt;= (Q3 + 1.5 * IQR))]\n\n\n# OLS Regression\nX = df_outliers_removed[['trip_duration_min','driver_rating', 'service_fee_to_app', 'passenger_count', 'fuel_cost_per_km']]\ny = df_outliers_removed['fare_amount_euro']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n#Too many influential data points were removed, making the remaining data too uniform, with removal of outliers,  coefficients for other variables are extremely close to zero, suggests that the model isn't really using those variables to predict fare\n#used df and not df with outerliers removed then\n#through vif calculation at end, found that trip distance and fuel_Cost_per_km were multicolinear, removed one in the analysis\n\n\n#when your trip duration to price, service_fee_to_app, fuel cost per km related to price\n#less than &gt;0.005 means it is significant\n#every unit of trip_duration_min fare_amount_euro increases by 0.16 euro (16 cents)\n#every unit of service_fee_to_app increases by 1.07euro\n#every unit of fuel_cost_per_km increases by 4.80euro\n\n\n#Plot fitted values vs distance\nfig = sm.graphics.plot_fit(model, \"fuel_cost_per_km\")\nfig.show()\n\n\nfrom statsmodels.stats.diagnostic import het_white\n# White test for heteroscedasticity\n\nwhite_test = het_white(model.resid, model.model.exog)\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nprint(dict(zip(labels, white_test)))\n\n\n#took out trip_distance_km due to multicolinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nX = df_outliers_removed[['trip_duration_min', 'service_fee_to_app', 'passenger_count', 'fuel_cost_per_km' ]]\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [vif(X.values, i) for i in range(len(X.columns))]\n\nprint(vif_data)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "code_projects/product_proposal/Uber Analysis(1).html",
    "href": "code_projects/product_proposal/Uber Analysis(1).html",
    "title": "Importing data and intial analysis",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport statsmodels.api as sm\nfrom statsmodels.stats.diagnostic import het_white\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n\n\n\n#Loaded the dataset & displayed first few rows\ndf = pd.read_csv('/content/UberDataset.csv')\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nSTART_DATE\nEND_DATE\nCATEGORY\nSTART\nSTOP\nMILES\nPURPOSE\n\n\n\n\n0\n01/01/2016 21:11\n01/01/2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01/02/2016 01:25\n01/02/2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01/02/2016 20:25\n01/02/2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01/05/2016 17:31\n01/05/2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01/06/2016 14:42\n01/06/2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n#Checkec for missing values & data types of columns\ndf.info()\n#no missing values\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1155 entries, 0 to 1154\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   START_DATE  1155 non-null   object \n 1   END_DATE    1155 non-null   object \n 2   CATEGORY    1155 non-null   object \n 3   START       1155 non-null   object \n 4   STOP        1155 non-null   object \n 5   MILES       1155 non-null   float64\n 6   PURPOSE     653 non-null    object \ndtypes: float64(1), object(6)\nmemory usage: 63.3+ KB\n\n\n\n# Standardize column names to lowercase for consistency\ndf.columns = df.columns.str.lower()\ndf\n\n\n  \n    \n\n\n\n\n\n\nstart_date\nend_date\ncategory\nstart\nstop\nmiles\npurpose\n\n\n\n\n0\n01/01/2016 21:11\n01/01/2016 21:17\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n\n\n1\n01/02/2016 01:25\n01/02/2016 01:37\nBusiness\nFort Pierce\nFort Pierce\n5.0\nNaN\n\n\n2\n01/02/2016 20:25\n01/02/2016 20:38\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n\n\n3\n01/05/2016 17:31\n01/05/2016 17:45\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n\n\n4\n01/06/2016 14:42\n01/06/2016 15:49\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1150\n12/31/2016 1:07\n12/31/2016 1:14\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n\n\n1151\n12/31/2016 13:24\n12/31/2016 13:42\nBusiness\nKar?chi\nUnknown Location\n3.9\nTemporary Site\n\n\n1152\n12/31/2016 15:03\n12/31/2016 15:38\nBusiness\nUnknown Location\nUnknown Location\n16.2\nMeeting\n\n\n1153\n12/31/2016 21:32\n12/31/2016 21:50\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n\n\n1154\n12/31/2016 22:08\n12/31/2016 23:51\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n\n\n\n\n1155 rows × 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Check for missing values in the dataset\ndf.isnull().sum()\n\n\n\n\n\n\n\n\n0\n\n\n\n\nstart_date\n0\n\n\nend_date\n0\n\n\ncategory\n0\n\n\nstart\n0\n\n\nstop\n0\n\n\nmiles\n0\n\n\npurpose\n502\n\n\n\n\ndtype: int64\n\n\n\n#Quick descriptive statistics of each numerical column\ndf.describe()\n\n\n  \n    \n\n\n\n\n\n\nmiles\n\n\n\n\ncount\n1155.000000\n\n\nmean\n10.566840\n\n\nstd\n21.579106\n\n\nmin\n0.500000\n\n\n25%\n2.900000\n\n\n50%\n6.000000\n\n\n75%\n10.400000\n\n\nmax\n310.300000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nData Cleaning\n\n#Checked & removed duplicates\nduplicated_data = df[df.duplicated()]\nprint(duplicated_data.shape)\n\ndf = df.drop_duplicates(keep='first')\n\n(1, 7)\n\n\n\n# Encode categorical variables using one-hot encoding\ndata_encoded = pd.get_dummies(df, columns=['category', 'purpose'], drop_first=True)\ndata_encoded\n\n\n  \n    \n\n\n\n\n\n\nstart_date\nend_date\nstart\nstop\nmiles\ncategory_Personal\npurpose_Between Offices\npurpose_Charity ($)\npurpose_Commute\npurpose_Customer Visit\npurpose_Errand/Supplies\npurpose_Meal/Entertain\npurpose_Meeting\npurpose_Moving\npurpose_Temporary Site\n\n\n\n\n0\n01/01/2016 21:11\n01/01/2016 21:17\nFort Pierce\nFort Pierce\n5.1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n1\n01/02/2016 01:25\n01/02/2016 01:37\nFort Pierce\nFort Pierce\n5.0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\n01/02/2016 20:25\n01/02/2016 20:38\nFort Pierce\nFort Pierce\n4.8\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\n01/05/2016 17:31\n01/05/2016 17:45\nFort Pierce\nFort Pierce\n4.7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n01/06/2016 14:42\n01/06/2016 15:49\nFort Pierce\nWest Palm Beach\n63.7\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1150\n12/31/2016 1:07\n12/31/2016 1:14\nKar?chi\nKar?chi\n0.7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1151\n12/31/2016 13:24\n12/31/2016 13:42\nKar?chi\nUnknown Location\n3.9\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1152\n12/31/2016 15:03\n12/31/2016 15:38\nUnknown Location\nUnknown Location\n16.2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1153\n12/31/2016 21:32\n12/31/2016 21:50\nKatunayake\nGampaha\n6.4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n1154\n12/31/2016 22:08\n12/31/2016 23:51\nGampaha\nIlukwatta\n48.2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n\n\n1154 rows × 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#convert date columns into date time\ndf[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors=\"coerce\")\ndf[\"end_date\"] = pd.to_datetime(df[\"end_date\"], errors=\"coerce\")\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"start_date\"] = pd.to_datetime(df[\"start_date\"], errors=\"coerce\")\n&lt;ipython-input-12-5a14aec24370&gt;:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"end_date\"] = pd.to_datetime(df[\"end_date\"], errors=\"coerce\")\n\n\n\n# Handle missing values by dropping rows with missing 'start' or 'stop' and filling 'purpose' with 'unknown'\ndf.dropna(subset=['start', 'stop'], inplace=True)\ndf[\"purpose\"].fillna(\"Unknown\", inplace=True)\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.dropna(subset=['start', 'stop'], inplace=True)\n&lt;ipython-input-13-5243f4142b4b&gt;:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['purpose'] = df['purpose'].fillna(\"Unknown\")\n\n\n\n# Create a new column for trip date\ndf[\"date\"] = df[\"start_date\"].dt.date\n\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"date\"] = df[\"start_date\"].dt.date\n\n\n\n# Ensure the dataset is sorted by date\ndf = df.sort_values('start_date')\ndf.set_index('start_date', inplace=True)\n\n\n\nAnalysis\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nend_date\ncategory\nstart\nstop\nmiles\npurpose\ndate\n\n\nstart_date\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01 21:11:00\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n2016-01-01\n\n\n2016-01-02 01:25:00\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nUnknown\n2016-01-02\n\n\n2016-01-02 20:25:00\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n2016-01-02\n\n\n2016-01-05 17:31:00\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n2016-01-05\n\n\n2016-01-06 14:42:00\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n2016-01-06\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Plot trip frequency over time\nplt.figure(figsize=(15, 8))\nsns.lineplot(x=df[\"date\"].value_counts().sort_index().index,\n             y=df[\"date\"].value_counts().sort_index().values, marker=\"o\")\nplt.xlabel(\"date\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Uber Trips Over Time\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Top 10 most common pickup and drop-off locations\ntop_pickups = df[\"start\"].value_counts().head(10)\ntop_dropoffs = df[\"stop\"].value_counts().head(10)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nsns.barplot(x=top_pickups.values, y=top_pickups.index, ax=axes[0], palette=\"Blues_r\")\naxes[0].set_title(\"Top 10 Pickup Locations\")\naxes[0].set_xlabel(\"Number of Trips\")\nsns.barplot(x=top_dropoffs.values, y=top_dropoffs.index, ax=axes[1], palette=\"Greens_r\")\naxes[1].set_title(\"Top 10 Drop-off Locations\")\naxes[1].set_xlabel(\"Number of Trips\")\nplt.tight_layout()\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_pickups.values, y=top_pickups.index, ax=axes[0], palette=\"Blues_r\")\n&lt;ipython-input-15-38d82e6eabd8&gt;:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=top_dropoffs.values, y=top_dropoffs.index, ax=axes[1], palette=\"Greens_r\")\n\n\n\n\n\n\n\n\n\n\n#Plotted count of trips by purpose\ntrip_purpose_counts = df[\"purpose\"].value_counts()\nplt.figure(figsize=(12, 5))\nsns.barplot(x=trip_purpose_counts.index, y=trip_purpose_counts.values, palette=\"Oranges\")\nplt.xlabel(\"Purpose of Trip\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Trip Purpose Distribution\")\nplt.xticks(rotation=45)\nplt.show()\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=trip_purpose_counts.index, y=trip_purpose_counts.values, palette=\"Oranges\")\n\n\n\n\n\n\n\n\n\n\n# Category-wise trip distribution\nplt.figure(figsize=(8, 5))\nsns.countplot(x=df[\"category\"], palette=\"viridis\")\nplt.xlabel(\"Trip Category\")\nplt.ylabel(\"Number of Trips\")\nplt.title(\"Business vs Personal Trips\")\nplt.show()\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(x=df[\"category\"], palette=\"viridis\")\n\n\n\n\n\n\n\n\n\n\n# Plot average trip distance by purpose\navg_miles_purpose = df.groupby(\"purpose\")[\"miles\"].mean().sort_values()\nplt.figure(figsize=(12, 5))\nsns.barplot(x=avg_miles_purpose.index, y=avg_miles_purpose.values, palette=\"Blues\")\nplt.xlabel(\"Purpose of Trip\")\nplt.ylabel(\"Average Miles Traveled\")\nplt.title(\"Average Trip Distance by Purpose\")\nplt.xticks(rotation=45)\nplt.show()\n\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x=avg_miles_purpose.index, y=avg_miles_purpose.values, palette=\"Blues\")\n\n\n\n\n\n\n\n\n\n\ndf\n\n\n  \n    \n\n\n\n\n\n\nend_date\ncategory\nstart\nstop\nmiles\npurpose\ndate\nhour\n\n\nstart_date\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01 21:11:00\n2016-01-01 21:17:00\nBusiness\nFort Pierce\nFort Pierce\n5.1\nMeal/Entertain\n2016-01-01\n21\n\n\n2016-01-02 01:25:00\n2016-01-02 01:37:00\nBusiness\nFort Pierce\nFort Pierce\n5.0\nUnknown\n2016-01-02\n1\n\n\n2016-01-02 20:25:00\n2016-01-02 20:38:00\nBusiness\nFort Pierce\nFort Pierce\n4.8\nErrand/Supplies\n2016-01-02\n20\n\n\n2016-01-05 17:31:00\n2016-01-05 17:45:00\nBusiness\nFort Pierce\nFort Pierce\n4.7\nMeeting\n2016-01-05\n17\n\n\n2016-01-06 14:42:00\n2016-01-06 15:49:00\nBusiness\nFort Pierce\nWest Palm Beach\n63.7\nCustomer Visit\n2016-01-06\n14\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2016-12-31 01:07:00\n2016-12-31 01:14:00\nBusiness\nKar?chi\nKar?chi\n0.7\nMeeting\n2016-12-31\n1\n\n\n2016-12-31 13:24:00\n2016-12-31 13:42:00\nBusiness\nKar?chi\nUnknown Location\n3.9\nTemporary Site\n2016-12-31\n13\n\n\n2016-12-31 15:03:00\n2016-12-31 15:38:00\nBusiness\nUnknown Location\nUnknown Location\n16.2\nMeeting\n2016-12-31\n15\n\n\n2016-12-31 21:32:00\n2016-12-31 21:50:00\nBusiness\nKatunayake\nGampaha\n6.4\nTemporary Site\n2016-12-31\n21\n\n\n2016-12-31 22:08:00\n2016-12-31 23:51:00\nBusiness\nGampaha\nIlukwatta\n48.2\nTemporary Site\n2016-12-31\n22\n\n\n\n\n1154 rows × 8 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# applying scatter plot linear regression\nnumerical_features = ['hour']\n\nfor feature in numerical_features:\n  fig = px.scatter(df, x=feature, y='miles', trendline=\"ols\", title=f'Miles vs. {feature}')\n  fig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\n# Fit OLS model to 'miles' using 'hour' as the predictor\nX = sm.add_constant(df['hour'])\ny = df['miles']\n\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\n#no correlations\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  miles   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.4551\nDate:                Thu, 10 Apr 2025   Prob (F-statistic):              0.500\nTime:                        17:22:57   Log-Likelihood:                -5182.0\nNo. Observations:                1154   AIC:                         1.037e+04\nDf Residuals:                    1152   BIC:                         1.038e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          9.1887      2.140      4.293      0.000       4.989      13.388\nhour           0.0939      0.139      0.675      0.500      -0.179       0.367\n==============================================================================\nOmnibus:                     1520.504   Durbin-Watson:                   1.111\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           212621.860\nSkew:                           7.198   Prob(JB):                         0.00\nKurtosis:                      67.921   Cond. No.                         52.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Calculate and visualize the correlation matrix\nnumerical_features = ['hour']\n\ncorrelation_matrix = df[numerical_features + ['miles']].corr()\n\n# Display the correlation table\nprint(correlation_matrix)\n\n# Visualize the correlation matrix using a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix of Numerical Features and Miles')\nplt.show()\n\n           hour     miles\nhour   1.000000  0.019872\nmiles  0.019872  1.000000\n\n\n\n\n\n\n\n\n\n\n# Fit OLS model and plot regression results\nX = sm.add_constant(df['hour'])\ny = df['miles']\nmodel_mult = sm.OLS(y,X).fit()\n\nfig = sm.graphics.plot_regress_exog(model_mult, \"hour\")\nfig.show()\n\n\n\n\n\n\n\n\n\nfig = sm.graphics.plot_fit(model_mult, \"hour\")\nfig.show()\n\n\n\n\n\n\n\n\n\n# Perform White's test for heteroscedasticity\nwhite_test = het_white(model_mult.resid, model_mult.model.exog)\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nprint(dict(zip(labels, white_test)))\n\n{'LM Statistic': np.float64(1.459580805313476), 'LM-Test p-value': np.float64(0.48201000752034195), 'F-Statistic': np.float64(0.7288150068045792), 'F-Test p-value': np.float64(0.4827029092747416)}\n\n\n\n# Fit OLS model with heteroscedasticity-robust standard errors (HC3)\ny_het = df['miles']\nX_het = df[['hour']]\nX_het = sm.add_constant(X_het)\n\nmodel_het = sm.OLS(y_het, X_het).fit(cov_type='HC3')\n\n# Print the model summary\nprint(model_het.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  miles   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.4441\nDate:                Thu, 10 Apr 2025   Prob (F-statistic):              0.505\nTime:                        17:22:59   Log-Likelihood:                -5182.0\nNo. Observations:                1154   AIC:                         1.037e+04\nDf Residuals:                    1152   BIC:                         1.038e+04\nDf Model:                           1                                         \nCovariance Type:                  HC3                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          9.1887      1.999      4.598      0.000       5.272      13.106\nhour           0.0939      0.141      0.666      0.505      -0.182       0.370\n==============================================================================\nOmnibus:                     1520.504   Durbin-Watson:                   1.111\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           212621.860\nSkew:                           7.198   Prob(JB):                         0.00\nKurtosis:                      67.921   Cond. No.                         52.0\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n\n\n\n# Calculate Variance Inflation Factor (VIF) for numerical features\nnumerical_features = ['hour', 'miles']\nX_vif = df[numerical_features]\n\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_vif.columns\nvif_data[\"VIF\"] = [vif(X_vif.values, i) for i in range(len(X_vif.columns))]\n\nvif_data\n\n\n  \n    \n\n\n\n\n\n\nfeature\nVIF\n\n\n\n\n0\nhour\n1.22077\n\n\n1\nmiles\n1.22077\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Create returns for 'miles' & dropped any missing values\ndf['dmiles'] = df['miles'].transform(lambda x: (x - x.shift(1)) / x.shift(1) * 100)\ndf = df.dropna()\n\n\n# Plot ACF and PACF for 'dmiles'\nplot_acf(df['dmiles'], lags=12, zero=False)\nplt.title('ACF of Differenced Miles')\nplt.show()\n\nplot_pacf(df['dmiles'], lags=12, zero=False)\nplt.title('PACF of Differenced Miles')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Fit ARIMA model\n\nmodel = ARIMA(df['dmiles'], order=(1, 0, 1))\nres = model.fit()\nprint(res.summary())\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n\n\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                 dmiles   No. Observations:                 1153\nModel:                 ARIMA(1, 0, 1)   Log Likelihood               -8847.338\nDate:                Thu, 10 Apr 2025   AIC                          17702.676\nTime:                        17:23:01   BIC                          17722.876\nSample:                             0   HQIC                         17710.300\n                               - 1153                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        106.7033     31.734      3.362      0.001      44.506     168.901\nar.L1          0.1202      0.730      0.165      0.869      -1.310       1.551\nma.L1         -0.1845      0.728     -0.254      0.800      -1.611       1.242\nsigma2      2.705e+05   2807.015     96.367      0.000    2.65e+05    2.76e+05\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):           1397205.21\nProb(Q):                              0.99   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.42   Skew:                            11.58\nProb(H) (two-sided):                  0.00   Kurtosis:                       171.96\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\n# Forecast future values using arima model\nforecast = res.get_forecast(steps=30)\nforecast_ci = forecast.conf_int()\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning:\n\nNo supported index is available. Prediction results will be given with an integer index beginning at `start`.\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning:\n\nNo supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n\n\n\n\n# Calculate RMSE for the ARIMA model\nforecast_values = forecast.predicted_mean\nactual_values = df['dmiles'][-30:]   # Assuming the last 30 values are used for comparison\nrmse = np.sqrt(mean_squared_error(actual_values, forecast_values))\nprint(f'RMSE: {rmse}')\n\nRMSE: 174.78697926552772\n\n\n\n# Plot the forecast\nplt.figure(figsize=(10, 6))\nplt.plot(df.index, df['dmiles'], label='Observed')\nplt.plot(forecast.predicted_mean.index, forecast.predicted_mean, label='Forecast')\nplt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='k', alpha=0.2)\nplt.xlabel('Date')\nplt.ylabel('Differenced Miles')\nplt.title('Forecast vs Actual')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Perform granger causality test\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\nmaxlag = 2\ntest_result = grangercausalitytests(df[['miles', 'hour']], maxlag=maxlag, verbose=False)\np_values = [round(test_result[i+1][0]['ssr_chi2test'][1], 4) for i in range(maxlag)]\nprint(f'P-Values: {p_values}')\n\nP-Values: [np.float64(0.8064), np.float64(0.6221)]\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/stattools.py:1556: FutureWarning:\n\nverbose is deprecated since functions should not print results\n\n\n\n\n# Fit VAR model\nfrom statsmodels.tsa.api import VAR\n\nmodel = VAR(df[['dmiles', 'hour']])\nres = model.fit(maxlags=2)\nprint(res.summary())\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Thu, 10, Apr, 2025\nTime:                     17:23:02\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                    15.5384\nNobs:                     1151.00    HQIC:                   15.5111\nLog likelihood:          -12173.5    FPE:                5.36034e+06\nAIC:                      15.4945    Det(Omega_mle):     5.31407e+06\n--------------------------------------------------------------------\nResults for equation dmiles\n============================================================================\n               coefficient       std. error           t-stat            prob\n----------------------------------------------------------------------------\nconst           159.531304        65.357401            2.441           0.015\nL1.dmiles        -0.065231         0.029571           -2.206           0.027\nL1.hour          -3.202747         3.481962           -0.920           0.358\nL2.dmiles        -0.020175         0.029578           -0.682           0.495\nL2.hour           0.237831         3.472154            0.068           0.945\n============================================================================\n\nResults for equation hour\n============================================================================\n               coefficient       std. error           t-stat            prob\n----------------------------------------------------------------------------\nconst            11.454759         0.554159           20.671           0.000\nL1.dmiles         0.000234         0.000251            0.935           0.350\nL1.hour           0.248928         0.029523            8.432           0.000\nL2.dmiles         0.000014         0.000251            0.056           0.956\nL2.hour          -0.030465         0.029440           -1.035           0.301\n============================================================================\n\nCorrelation matrix of residuals\n            dmiles      hour\ndmiles    1.000000 -0.038083\nhour     -0.038083  1.000000\n\n\n\n\n\n/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n\nA date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n\n\n\n\n!pip install arch\n\nCollecting arch\n  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy&gt;=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (2.0.2)\nRequirement already satisfied: scipy&gt;=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.14.1)\nRequirement already satisfied: pandas&gt;=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.2)\nRequirement already satisfied: statsmodels&gt;=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=1.4-&gt;arch) (2025.2)\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (1.0.1)\nRequirement already satisfied: packaging&gt;=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels&gt;=0.12-&gt;arch) (24.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.4-&gt;arch) (1.17.0)\nDownloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 985.3/985.3 kB 12.4 MB/s eta 0:00:00\n\n\n\n# Fit GARCH model\nfrom arch import arch_model\nam = arch_model(df['dmiles'], vol='GARCH')\nres = am.fit()\nprint(res.summary())\n\n\n\n\n\n Back to top"
  }
]